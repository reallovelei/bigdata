### Work
一个物理节点可以有一个 或者多个worker. 一个work可以有一个或多个executor,一个executor拥有多个cpu core和memory.


### Job
A job is triggered by an action, like count() or saveAsTextFile(). Click on a job to see information about the stages of tasks inside it. 
理解了吗，所谓一个 job，就是由一个 rdd 的 **action** 触发的动作，可以简单的理解为，当你需要执行一个 rdd 的 action 的时候，会生成一个 job。一个**action算子** 就算一个Job, 如：count, first等。
 
### Stage
stage是一个job的组成单位，就是说，一个job会被切分成1个或1个以上的stage， 然后各个stage会按照执行顺序依次执行。。
(只有shuffle操作时 才算作一个stage。如:reduceByKey, groupby, join， union, collect  **产生宽依赖的** 那这里的算子怎么不切分job  不是action算子？)

### Task
 Task是Spark中任务的执行单元。RDD一般是带有partitions的，每个partition 对应 一个task。




### spark dataframe 和pandas dataframe互转。
![spark-dataframe-6.jpg](http://litaotao.github.io/images/spark-dataframe-6.jpg)

 
### spark dataframe 和 pandas dataframe 选择问题

如果数据量小，结构简单，可以直接用 pandas dataframe 来做分析；如果数据量大，结构复杂 [嵌套结构]，那么推荐用 spark dataframe 来做数据分析，然后把结果转成 pandas dataframe，用 pandas dataframe 来做展示和报告。



<!--stackedit_data:
eyJoaXN0b3J5IjpbNTAzMTk2NjFdfQ==
-->